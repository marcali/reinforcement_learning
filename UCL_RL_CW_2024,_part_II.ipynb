{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part II (25 pts total)\n",
        "---\n",
        "\n",
        "**Name:** Your Name\n",
        "\n",
        "**SN:** Your Student Number\n",
        "\n",
        "---\n",
        "\n",
        "**Due date:** *April 11th, 2024*\n",
        "\n",
        "---\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part2.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "## Context\n",
        "\n",
        "In this part, we will take a first look at learning algorithms for sequential decision problems.\n",
        "\n",
        "## Background reading\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 3 - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmoRG8jB-05-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "# The Assignment\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You will use Python to implement several reinforcement learning algorithms.\n",
        "\n",
        "You will then run these algorithms on a few problems, to understand their properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB0tQ4aiAaIu"
      },
      "source": [
        "### Import Useful Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.collections as mcoll\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NDhSYfSDcCC"
      },
      "source": [
        "### Set options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\alina\\AppData\\Local\\Temp\\ipykernel_9168\\4001635480.py:2: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-colorblind')\n"
          ]
        }
      ],
      "source": [
        "np.set_printoptions(precision=3, suppress=1)\n",
        "plt.style.use('seaborn-colorblind')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALrRR76eAd6u"
      },
      "source": [
        "### Some grid world"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "outputs": [],
      "source": [
        "W = -100  # wall\n",
        "G = 100  # goal\n",
        "\n",
        "GRID_LAYOUT = np.array([\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W],\n",
        "  [W, W, 0, W, W, W, W, W, W, 0, W, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, G, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, W, W, W, W, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, W],\n",
        "  [W, W, 0, 0, 0, 0, 0, 0, 0, 0, W, W],\n",
        "  [W, W, W, W, W, W, W, W, W, W, W, W]\n",
        "])\n",
        "\n",
        "class Grid(object):\n",
        "\n",
        "  def __init__(self, noisy=False):\n",
        "    # -1: wall\n",
        "    # 0: empty, episode continues\n",
        "    # other: number indicates reward, episode will terminate\n",
        "    self._layout = GRID_LAYOUT\n",
        "    self._start_state = (2, 2)\n",
        "    self._state = self._start_state\n",
        "    self._number_of_states = np.prod(np.shape(self._layout))\n",
        "    self._noisy = noisy\n",
        "\n",
        "  @property\n",
        "  def number_of_states(self):\n",
        "    return self._number_of_states\n",
        "\n",
        "  def get_obs(self):\n",
        "    y, x = self._state\n",
        "    return y*self._layout.shape[1] + x\n",
        "\n",
        "  def obs_to_state(self, obs):\n",
        "    x = obs % self._layout.shape[1]\n",
        "    y = obs // self._layout.shape[1]\n",
        "    s = np.copy(grid._layout)\n",
        "    s[y, x] = 4\n",
        "    return s\n",
        "\n",
        "  def step(self, action):\n",
        "    y, x = self._state\n",
        "\n",
        "    if action == 0:  # up\n",
        "      new_state = (y - 1, x)\n",
        "    elif action == 1:  # right\n",
        "      new_state = (y, x + 1)\n",
        "    elif action == 2:  # down\n",
        "      new_state = (y + 1, x)\n",
        "    elif action == 3:  # left\n",
        "      new_state = (y, x - 1)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "    new_y, new_x = new_state\n",
        "    reward = self._layout[new_y, new_x]\n",
        "    if self._layout[new_y, new_x] == W:  # wall\n",
        "      discount = 0.9\n",
        "      new_state = (y, x)\n",
        "    elif self._layout[new_y, new_x] == 0:  # empty cell\n",
        "      reward = -1.\n",
        "      discount = 0.9\n",
        "    else:  # a goal\n",
        "      discount = 0.\n",
        "      new_state = self._start_state\n",
        "\n",
        "    if self._noisy:\n",
        "      width = self._layout.shape[1]\n",
        "      reward += 10*np.random.normal(0, width - new_x + new_y)\n",
        "\n",
        "    self._state = new_state\n",
        "    return reward, discount, self.get_obs()\n",
        "\n",
        "  def plot_grid(self):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(self._layout != W, interpolation=\"nearest\", cmap='pink')\n",
        "    plt.gca().grid(0)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.title(\"The grid\")\n",
        "    plt.text(2, 2, r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "    plt.text(9, 2, r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "    h, w = self._layout.shape\n",
        "    for y in range(h-1):\n",
        "      plt.plot([-0.5, w-0.5], [y+0.5, y+0.5], '-k', lw=2)\n",
        "    for x in range(w-1):\n",
        "      plt.plot([x+0.5, x+0.5], [-0.5, h-0.5], '-k', lw=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOu9RZY3AkF1"
      },
      "source": [
        "### Helper functions\n",
        "(You should not have to change, or even look at, these.  Do run the cell to make sure the functions are loaded though.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6EttQGJ1n5Zn"
      },
      "outputs": [],
      "source": [
        "def run_experiment(env, agent, number_of_steps):\n",
        "  mean_reward = 0.\n",
        "  try:\n",
        "    action = agent.initial_action()\n",
        "  except AttributeError:\n",
        "    action = 0\n",
        "  for _ in range(number_of_steps):\n",
        "    reward, discount, next_state = env.step(action)\n",
        "    action = agent.step(reward, discount, next_state)\n",
        "    mean_reward += reward\n",
        "  return mean_reward/float(number_of_steps)\n",
        "\n",
        "map_from_action_to_subplot = lambda a: (2, 6, 8, 4)[a]\n",
        "map_from_action_to_name = lambda a: (\"up\", \"right\", \"down\", \"left\")[a]\n",
        "\n",
        "def plot_values(grid, values, colormap='pink', vmin=0, vmax=10):\n",
        "  plt.imshow(values - 1000*(grid<0), interpolation=\"nearest\", cmap=colormap, vmin=vmin, vmax=vmax)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  plt.colorbar(ticks=[vmin, vmax])\n",
        "\n",
        "def plot_action_values(grid, action_values, vmin=-5, vmax=5):\n",
        "  q = action_values\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "  for a in [0, 1, 2, 3]:\n",
        "    plt.subplot(4, 3, map_from_action_to_subplot(a))\n",
        "    plot_values(grid, q[..., a], vmin=vmin, vmax=vmax)\n",
        "    action_name = map_from_action_to_name(a)\n",
        "    plt.title(r\"$q(s, \\mathrm{\" + action_name + r\"})$\")\n",
        "\n",
        "  plt.subplot(4, 3, 5)\n",
        "  v = np.max(q, axis=-1)\n",
        "  plot_values(grid, v, colormap='summer', vmin=vmin, vmax=vmax)\n",
        "  plt.title(\"$v(s)$\")\n",
        "\n",
        "  # Plot arrows:\n",
        "  plt.subplot(4, 3, 11)\n",
        "  plot_values(grid, grid==0, vmax=1)\n",
        "  for row in range(len(grid)):\n",
        "    for col in range(len(grid[0])):\n",
        "      if grid[row][col] == 0:\n",
        "        argmax_a = np.argmax(q[row, col])\n",
        "        if argmax_a == 0:\n",
        "          x = col\n",
        "          y = row + 0.5\n",
        "          dx = 0\n",
        "          dy = -0.8\n",
        "        if argmax_a == 1:\n",
        "          x = col - 0.5\n",
        "          y = row\n",
        "          dx = 0.8\n",
        "          dy = 0\n",
        "        if argmax_a == 2:\n",
        "          x = col\n",
        "          y = row - 0.5\n",
        "          dx = 0\n",
        "          dy = 0.8\n",
        "        if argmax_a == 3:\n",
        "          x = col + 0.5\n",
        "          y = row\n",
        "          dx = -0.8\n",
        "          dy = 0\n",
        "        plt.arrow(x, y, dx, dy, width=0.02, head_width=0.4, head_length=0.4, length_includes_head=True, fc='k', ec='k')\n",
        "\n",
        "def plot_rewards(xs, rewards, color):\n",
        "  mean = np.mean(rewards, axis=0)\n",
        "  p90 = np.percentile(rewards, 90, axis=0)\n",
        "  p10 = np.percentile(rewards, 10, axis=0)\n",
        "  plt.plot(xs, mean, color=color, alpha=0.6)\n",
        "  plt.fill_between(xs, p90, p10, color=color, alpha=0.3)\n",
        "\n",
        "def parameter_study(parameter_values, parameter_name, agent_constructor,\n",
        "                    env_constructor, color,\n",
        "                    repetitions=10, number_of_steps=int(1e4)):\n",
        "  mean_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  greedy_rewards = np.zeros((repetitions, len(parameter_values)))\n",
        "  for rep in range(repetitions):\n",
        "    for i, p in enumerate(parameter_values):\n",
        "      env = env_constructor()\n",
        "      agent = agent_constructor()\n",
        "      if 'eps' in parameter_name:\n",
        "        agent.set_epsilon(p)\n",
        "      elif 'alpha' in parameter_name:\n",
        "        agent._step_size = p\n",
        "      else:\n",
        "        raise NameError(\"Unknown parameter_name: {}\".format(parameter_name))\n",
        "      mean_rewards[rep, i] = run_experiment(env, agent, number_of_steps)\n",
        "      agent.set_epsilon(0.)\n",
        "      agent._step_size = 0.\n",
        "      greedy_rewards[rep, i] = run_experiment(env, agent, number_of_steps//10)\n",
        "      del env\n",
        "      del agent\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plot_rewards(parameter_values, mean_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Average reward over first {} steps\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plot_rewards(parameter_values, greedy_rewards, color)\n",
        "  plt.yticks=([0, 1], [0, 1])\n",
        "  plt.ylabel(\"Final rewards, with greedy policy\".format(number_of_steps), size=12)\n",
        "  plt.xlabel(parameter_name, size=12)\n",
        "\n",
        "def epsilon_greedy(q_values, epsilon):\n",
        "  if epsilon < np.random.random():\n",
        "    return np.argmax(q_values)\n",
        "  else:\n",
        "    return np.random.randint(np.array(q_values).shape[-1])\n",
        "\n",
        "\n",
        "def colorline(x, y, z):\n",
        "  \"\"\"\n",
        "  Based on:\n",
        "  http://nbviewer.ipython.org/github/dpsanders/matplotlib-examples/blob/master/colorline.ipynb\n",
        "  http://matplotlib.org/examples/pylab_examples/multicolored_line.html\n",
        "  Plot a colored line with coordinates x and y\n",
        "  Optionally specify colors in the array z\n",
        "  Optionally specify a colormap, a norm function and a line width\n",
        "  \"\"\"\n",
        "  segments = make_segments(x, y)\n",
        "  lc = mcoll.LineCollection(segments, array=z, cmap=plt.get_cmap('copper_r'),\n",
        "                            norm=plt.Normalize(0.0, 1.0), linewidth=3)\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.add_collection(lc)\n",
        "  return lc\n",
        "\n",
        "\n",
        "def make_segments(x, y):\n",
        "  \"\"\"\n",
        "  Create list of line segments from x and y coordinates, in the correct format\n",
        "  for LineCollection: an array of the form numlines x (points per line) x 2 (x\n",
        "  and y) array\n",
        "  \"\"\"\n",
        "  points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
        "  segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
        "  return segments\n",
        "\n",
        "\n",
        "def plotting_helper_function(_x, _y, title=None, ylabel=None):\n",
        "  z = np.linspace(0, 0.9, len(_x))**0.7\n",
        "  colorline(_x, _y, z)\n",
        "  plt.plot(0, 0, '*', color='#000000', ms=20, alpha=0.7, label='$w^*$')\n",
        "  plt.plot(1, 1, '.', color='#ee0000', alpha=0.7, ms=20, label='$w_0$')\n",
        "  min_y, max_y = np.min(_y), np.max(_y)\n",
        "  min_x, max_x = np.min(_x), np.max(_x)\n",
        "  min_y, max_y = np.min([0, min_y]), np.max([0, max_y])\n",
        "  min_x, max_x = np.min([0, min_x]), np.max([0, max_x])\n",
        "  range_y = max_y - min_y\n",
        "  range_x = max_x - min_x\n",
        "  max_range = np.max([range_y, range_x])\n",
        "  plt.arrow(_x[-3], _y[-3], _x[-1] - _x[-3], _y[-1] - _y[-3], color='k',\n",
        "            head_width=0.04*max_range, head_length=0.04*max_range,\n",
        "            head_starts_at_zero=False)\n",
        "  plt.ylim(min_y - 0.2*range_y, max_y + 0.2*range_y)\n",
        "  plt.xlim(min_x - 0.2*range_x, max_x + 0.2*range_x)\n",
        "  ax = plt.gca()\n",
        "  ax.ticklabel_format(style='plain', useMathText=True)\n",
        "  plt.legend(loc=2)\n",
        "  plt.xticks(rotation=12, fontsize=10)\n",
        "  plt.yticks(rotation=12, fontsize=10)\n",
        "  plt.locator_params(nbins=3)\n",
        "  if title is not None:\n",
        "    plt.title(title, fontsize=20)\n",
        "  if ylabel is not None:\n",
        "    plt.ylabel(ylabel, fontsize=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# Section 1: Tabular RL\n",
        "\n",
        "In this section, observations will be states in the environment, so the agent state, environment state, and observation will all be the same, and we will use the word `state` interchangably with `observation`.  You will implement agents, which should be in pure Python - so you cannot use JAX/TensorFlow/PyTorch to compute gradients. Using `numpy` is fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Z5IgXfU2Qw"
      },
      "source": [
        "### A random agent\n",
        "\n",
        "Below we show a reference implementation of a simple random agent, implemented according to the interface above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mf64o3b3U6A4"
      },
      "outputs": [],
      "source": [
        "class Random(object):\n",
        "\n",
        "  def __init__(self, number_of_actions, number_of_states, initial_state):\n",
        "    self._number_of_actions = number_of_actions\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    next_action = np.random.randint(self._number_of_actions)\n",
        "    return next_action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaGeLcsvixmt"
      },
      "source": [
        "### The grid\n",
        "\n",
        "The cell below shows the `Grid` environment that we will use in this section. Here `S` indicates the start state and `G` indicates the goal.  The agent has four possible actions: up, right, down, and left.  Rewards are: `-100` for bumping into a wall, `+100` for reaching the goal, and `-1` otherwise.  The episode ends when the agent reaches the goal, and otherwise continues.  The discount, on continuing steps, is $\\gamma = 0.9$.  Feel free to reference the implemetation of the `Grid` above, under the header \"a grid world\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SlFuWFzIi5uB"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "GRID = Grid()\n",
        "GRID.plot_grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8oKd0oyvNcH"
      },
      "source": [
        "\n",
        "## Q1: Implement TD learning **[4 pts]**\n",
        "Implement an agent that acts randomly, and _on-policy_ estimates state values $v(s)$, using one-step TD learning with step size $\\alpha=0.1$.\n",
        "\n",
        "Use the `__init__` as provided below.  You should implement the `step` function.  We store the initial state in the constructor because you need its value on the first `step` in order to compute the TD error when the first transition has occurred.\n",
        "\n",
        "Also implement a property `state_values(self)` returning the vector of all state values (one value per state). (A method with the `@property` decorator can be called without the parentheses as if it's a variable, e.g., `agent.state_values` instead of `agent.state_values()`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4yKt1qYYjWTR"
      },
      "outputs": [],
      "source": [
        "class RandomTD(object):\n",
        "\n",
        "  def __init__(self, number_of_states, number_of_actions, initial_state, step_size=0.1):\n",
        "    self.number_of_states = number_of_states\n",
        "    self.number_of_actions = number_of_actions\n",
        "    self.initial_state = initial_state\n",
        "    self.step_size = step_size\n",
        "    self.values = np.zeros(self.number_of_states)\n",
        "\n",
        "  @property\n",
        "  def state_values(self):\n",
        "    \"\"\"\n",
        "    This property represents the state values of the system.\n",
        "\n",
        "    It returns a vector where each element is the value of a particular state. \n",
        "    The value of a state is a measure of the expected future reward from that state, \n",
        "    under the current policy.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: A vector of state values. The size of the vector is equal to \n",
        "        the number of states in the system.\n",
        "    \"\"\"\n",
        "    return self.values\n",
        "\n",
        "  def step(self, reward, discount, next_state):\n",
        "    if reward == None:\n",
        "      #initialize state\n",
        "      current_state = self.initial_state\n",
        "    else:\n",
        "      #set up next state\n",
        "      current_state = next_state\n",
        "    self.values[current_state] += self.step_size*(reward+(discount*self.values[next_state])- self.values[current_state])\n",
        "    #acts randomly\n",
        "    action = np.random.randint(self.number_of_actions)\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaMmp1lDgpUG"
      },
      "source": [
        "### Run the next cell to run the `RandomTD` agent on a grid world.\n",
        "\n",
        "If everything worked as expected, the plot below will show the estimates state values under the random policy. This includes values for unreachable states --- on the walls and on the goal (we never actually reach the goal --- rather, the episode terminates on the transition to the goal.  The values on the walls and goal are, and will always remain, zero. The plotting code knows about the walls, so they appear black below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "N0ZoYwgZfho2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Do not modify this cell.\n",
        "AGENT = RandomTD(GRID._layout.size, 4, GRID.get_obs())\n",
        "run_experiment(GRID, AGENT, int(1e5))\n",
        "v = AGENT.state_values\n",
        "plot_values(GRID_LAYOUT,\n",
        "            v.reshape(GRID._layout.shape),\n",
        "            colormap=\"pink\", vmin=-400, vmax=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxc_Sx7og4JH"
      },
      "source": [
        "## Q2: Policy iteration **[3 pts]**\n",
        "We used TD to do policy evaluation for the random policy on this problem.  Consider doing policy improvement, by taking the greedy policy with respect to a one-step look-ahead.  For this, you may assume we have a true model, so for each state and for each action we can look at the value of the resulting state, and would then pick the action with the highest reward plus subsequent state value. In other words, you can assume we can use $q(s, a) = \\mathbb{E}[ R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s, A_t = a]$, where $v$ is the value function learned by TD as implemented. Then we consider the policy that picks the action with the highest action value $q(s, a)$. You do **not** have to implement this, just answer the following question.\n",
        "\n",
        "The above amounts to performing an iteration of policy evaluation and policy improvement.  If we repeat this process over and over again, and repeatedly evaluate the greedy policy and then perform an improvement step by picking the greedy policy, would the policy eventually become optimal?  Explain why or why not in at most three sentences.\n",
        "\n",
        "> *Answer here*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R58NP87jbM"
      },
      "source": [
        "# Section 2: Off-policy Bellman operators with function approximation\n",
        "# _(40 pts total for the whole section)_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIyALFeoiov9"
      },
      "source": [
        "## Q2: Bellman operator for prediction **[5 pts]**\n",
        "\n",
        "We are going to implement the _expected_ update to the weight for a simple MDP.\n",
        "In order to be able to implement this update, we need to know the MDP, as well as the way the agent's value depend on the weights and states.  This is defined as follows:\n",
        "\n",
        "There are two states, $s_1$ and $s_2$.  All rewards are zero, and therefore can be ignored.  Instead of the underlying state, the agent observes state features $\\mathbf{x}_1 = \\mathbf{x}(s_1)$ and $\\mathbf{x}_2 = \\mathbf{x}(s_2)$.  These are defined for the two states are $\\mathbf{x}_1 = [1, 1]^{\\top}$ and $\\mathbf{x}_2 = [2, 1]^{\\top}$.  In each state, there are two actions, $a$ and $b$.  Action $a$ always transitions to state $s_1$, action $b$ always transitions to state $s_2$.\n",
        "\n",
        "![MDP](https://hadovanhasselt.files.wordpress.com/2020/02/mdp.png)\n",
        "\n",
        "---\n",
        "\n",
        "The agent will make *linear* predicions, such that $v(s) = \\mathbf{w}^\\top \\mathbf{x}(s)$. In the code cell below, you should implement an update that computes the **expected** weight update, given:\n",
        "* The current weights `w`.\n",
        "* A target policy $\\pi$ that we want to learn about; the actual input, denoted `pi` in the code below, will be a scalar indicating the probability of selecting action `a` in both states: `pi` $= \\pi(a|s), \\forall s$.\n",
        "* A behaviour policy $\\mu$ that would be used to generate the transitions; the actual input, denoted `mu` in the code below, will be a scalar indicating the probability of selecting action `a` in both states: `mu` $= \\mu(a|s), \\forall s$.\n",
        "* A scalar trace parameter $\\lambda$ (=`trace_parameter`),\n",
        "* A scalar discount parameter $\\gamma$ (=`discount`).\n",
        "\n",
        "The expectation should take into account the probabilities of actions in the future, as well as the steady-state (=long-term) probability of being in a state.  The step size of the update should be $\\alpha=0.1$.\n",
        "\n",
        "The expected update should be for a multi-step $\\lambda$-return and should be correct for any pair of target and any behaviour policy.  It should be the expectation of performing a (forward view) TD($\\lambda$) update in the MDP described above when the _state distribution_ is generated by the behaviour policy and the _returns_ from each state are generated by the target policy. We will use the update you implement to generate plots below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\Delta w_t = \\alpha (G_t^{\\lambda} - v_w(S_t)) \\nabla_w v_w(S_t) $$\n",
        "\n",
        "$$G_t^{\\lambda} = R_{t+1} + \\gamma ((1 - \\lambda)v_w(S_{t+1}) + \\lambda G_{t+1}^{\\tilde{}}) $$\n",
        "$$= R_{t+1} + \\gamma (1 - \\lambda)v_w(S_{t+1}) + \\lambda (R_{t+2} + \\gamma ((1 - \\lambda)v_w(S_{t+2}) +  \\lambda (R_{t+3} + \\gamma ((1 - \\lambda)v_w(S_{t+3}) +  \\lambda (....))) ))$$\n",
        "$$= R_{t+1} + \\gamma(1 - \\lambda)v_w(S_{t+1}) + \\gamma \\lambda R_{t+2} + \\gamma^2 \\lambda((1 - \\lambda))v_w(S_{t+2})+\\gamma^2 \\lambda^2 R_{t+3} + \\gamma^3 \\lambda^2((1 - \\lambda))v_w(S_{t+3})+..  $$\n",
        "Rewards 0:\n",
        "$$ \\sum_{n=1}^{\\infty} R_{t+n} = 0 $$\n",
        "\n",
        "Write as infinite sum\n",
        "$$G_t^{\\lambda} = \\sum_{n=0}^{\\infty} (\\gamma\\lambda )^n + \\sum_{n=0}^{\\infty} \\gamma^{n+1} \\lambda^n (1 - \\lambda) v_w(S_{t+n+1})$$\n",
        "\n",
        "\n",
        "Infinite geometric sum\n",
        "$$ S = A + \\sum_{n=1}^{\\infty} a r^{n-1}$$\n",
        "$$ S = A + \\frac{a}{1 - r}$$\n",
        "Applying\n",
        "$$S = \\frac{(\\gamma (1 - \\lambda) v_w(S_{t+n+1}))}{1 - \\gamma\\lambda}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sEpxlyUtrj3i"
      },
      "outputs": [],
      "source": [
        "# state features (do not change)\n",
        "x1 = np.array([1., 1.])\n",
        "x2 = np.array([2., 1.])\n",
        "\n",
        "def expected_update(w, pi, mu, trace_parameter, discount, lr):\n",
        "  v_1 = w.T @ x1\n",
        "  v_2 = w.T @ x2\n",
        "  v_w = v_1 + v_2\n",
        "  #write as a finate sum not recursive\n",
        "  \n",
        "  # lambda returns generated by taget policy pi\n",
        "  G_t = (discount*(1-trace_parameter)*(v_1*pi + v_2*(1-pi)))/(1-discount*trace_parameter)\n",
        "   \n",
        "  #exp update multi-step trace return, forward view TD(trace)\n",
        "  delta1 = G_t - v_1\n",
        "  weight_update1 = lr * delta1 * x1\n",
        "  delta2 = G_t - v_2\n",
        "  weight_update2 = lr * delta2 * x2\n",
        "  #state distribution generated by the behaviour policy mu\n",
        "  expected_weight_update = weight_update1*mu + weight_update2*(1-mu)\n",
        "\n",
        "  return expected_weight_update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U42IsCPW0KGY"
      },
      "source": [
        "##Experiment 3: run the cell below\n",
        "The cell below runs an experiment, across different target policies and trace parameters $\\lambda$.\n",
        "\n",
        "The plots below the cell will show how the weights move within the 2-dimensional weight space, starting from $w_0 = [1, 1]^{\\top}$ (shown as red dot).  The optimal solution $w_* = [0, 0]^{\\top}$ is also shown (as black star)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OTFFpQSX0Eaj"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;28mlen\u001b[39m(pis), \u001b[38;5;28mlen\u001b[39m(lambdas), r\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(lambdas) \u001b[38;5;241m+\u001b[39m c \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(x1)\n\u001b[1;32m---> 22\u001b[0m ws \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_ws\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mlambda=\u001b[39m\u001b[38;5;132;01m{:1.3f}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(l) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     24\u001b[0m ylabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpi=\u001b[39m\u001b[38;5;132;01m{:1.1f}\u001b[39;00m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pi) \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[11], line 5\u001b[0m, in \u001b[0;36mgenerate_ws\u001b[1;34m(w, pi, mu, l, g)\u001b[0m\n\u001b[0;32m      3\u001b[0m ws \u001b[38;5;241m=\u001b[39m [w]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m   w \u001b[38;5;241m=\u001b[39m w \u001b[38;5;241m+\u001b[39m \u001b[43mexpected_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m   ws\u001b[38;5;241m.\u001b[39mappend(w)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(ws)\n",
            "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mexpected_update\u001b[1;34m(w, pi, mu, trace_parameter, discount, lr)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpected_update\u001b[39m(w, pi, mu, trace_parameter, discount, lr):\n\u001b[0;32m      6\u001b[0m   v_1 \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m x1\n\u001b[1;32m----> 7\u001b[0m   v_2 \u001b[38;5;241m=\u001b[39m \u001b[43mw\u001b[49m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m x2\n\u001b[0;32m      8\u001b[0m   v_w \u001b[38;5;241m=\u001b[39m v_1 \u001b[38;5;241m+\u001b[39m v_2\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;66;03m#write as a finate sum not recursive\u001b[39;00m\n\u001b[0;32m     10\u001b[0m   \n\u001b[0;32m     11\u001b[0m   \u001b[38;5;66;03m# lambda returns generated by taget policy pi\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mexpected_update\u001b[1;34m(w, pi, mu, trace_parameter, discount, lr)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpected_update\u001b[39m(w, pi, mu, trace_parameter, discount, lr):\n\u001b[0;32m      6\u001b[0m   v_1 \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m x1\n\u001b[1;32m----> 7\u001b[0m   v_2 \u001b[38;5;241m=\u001b[39m \u001b[43mw\u001b[49m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m x2\n\u001b[0;32m      8\u001b[0m   v_w \u001b[38;5;241m=\u001b[39m v_1 \u001b[38;5;241m+\u001b[39m v_2\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;66;03m#write as a finate sum not recursive\u001b[39;00m\n\u001b[0;32m     10\u001b[0m   \n\u001b[0;32m     11\u001b[0m   \u001b[38;5;66;03m# lambda returns generated by taget policy pi\u001b[39;00m\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\alina\\miniconda3\\envs\\MV00\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\alina\\miniconda3\\envs\\MV00\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2200x1700 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def generate_ws(w, pi, mu, l, g):\n",
        "  \"\"\"Apply the expected update 1000 times\"\"\"\n",
        "  ws = [w]\n",
        "  for _ in range(1000):\n",
        "    w = w + expected_update(w, pi, mu, l, g, lr=0.1)\n",
        "    ws.append(w)\n",
        "  return np.array(ws)\n",
        "\n",
        "mu = 0.5  # behaviour\n",
        "g = 0.99  # discount\n",
        "\n",
        "lambdas = np.array([0, 0.8, 0.9, 0.95, 1.])\n",
        "pis = np.array([0., 0.1, 0.2, 0.5, 1.])\n",
        "\n",
        "fig = plt.figure(figsize=(22, 17))\n",
        "fig.subplots_adjust(wspace=0.25, hspace=0.3)\n",
        "\n",
        "for r, pi in enumerate(pis):\n",
        "  for c, l in enumerate(lambdas):\n",
        "    plt.subplot(len(pis), len(lambdas), r*len(lambdas) + c + 1)\n",
        "    w = np.ones_like(x1)\n",
        "    ws = generate_ws(w, pi, mu, l, g)\n",
        "    title = '$\\\\lambda={:1.3f}$'.format(l) if r == 0 else None\n",
        "    ylabel = '$\\\\pi={:1.1f}$'.format(pi) if c == 0 else None\n",
        "    plotting_helper_function(ws[:, 0], ws[:, 1], title, ylabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KxL4o357_dt"
      },
      "source": [
        "## Q8: Analyse results (13 pts total)\n",
        "1. **[1 pts]** How many of the above 25 experiments diverge?\n",
        "1. **[1 pts]** For which policies $\\pi$, is the true value function $v_{\\pi}$ representable in the above feature space (spanned by $x_1, x_2$).\n",
        "1. **[2 pts]** Why are the results asymmetric across different $\\pi$?  In particular, explain why the results look different when comparing $\\pi = \\pi(a | \\cdot) = 0$ to $\\pi(a | \\cdot) = 1$.\n",
        "1. **[2 pts]** For which combination of $\\pi(a)$ and $\\lambda$ does the expected update (with uniform random behaviour) converge? Do not limit the answer to the subset of values in the plots above, but to all possible choices of $\\lambda \\in [0, 1]$ and $\\pi(a|s) \\in [0, 1]$, but do restrict yourself to state-less policies, as above, for which the action probabilities are equal in the two states: $\\pi(a|s_1) = \\pi(a|s_2)$.\n",
        "1. **[1 pts]** Why do all the plots corresponding to full Monte Carlo update look the same (right column)?\n",
        "1. **[2 pts]** Why do the plots corresponding to full Monte Carlo have the shape they do?\n",
        "1. **[2 pts]** The plots above are for uniform behavoiur: $\\mu(a|s) = \\mu(b|s) = 0.5$.  How would the results change (high level, not in terms of precise plots) if the behaviour policy $\\mu$ would select action $a$ more often (e.g., $\\mu = 0.8$)?  How would the results change if the behaviour would select $a$ less often (e.g., $\\mu = 0.2$)?\n",
        "1. **[2 pts]** Consider again the orginal experiment, where data is gathered under uniformly random behaviour policy. What would the updates to the vectors $w$ be under the $L_\\infty$ norm? You can either run the experiment or give the closed-form update in an equation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0pm2-4rvB49"
      },
      "source": [
        "## Put answers to Q8 in this cell:\n",
        "1. 7 experiments are diverging, $\\pi$ = 0.0 and $\\lambda$ s 0.0, 0.8 and 0.9, $\\pi$=0.1 and $\\lambda$ s 0.0 and 0.8, $\\pi$ = 0.0 and $\\lambda$ = 0.0\n",
        "2. All policies pi are representable in feature space spanned by $x_1, x_2$ of the true value function $v_{\\pi}$, there are cases of convergence for all $\\pi$ with specific $\\lambda$, so the true value function is represented but depends on $\\lambda$\n",
        "3. So the general expression for the return $G_t^{\\lambda} = \\frac{(\\gamma (1 - \\lambda) v_w(S_{t+n+1}))}{1 - \\gamma\\lambda} = \\frac{(\\gamma (1 - \\lambda) (v_{1}\\pi + v_{2}(1-\\pi)))}{1 - \\gamma\\lambda} $ \\\n",
        "and  $ v_{1} = \\mathbf{w}^\\top \\mathbf{x_{1}} = 2$, $ v_{2} = \\mathbf{w}^\\top \\mathbf{x_{2}} =3$, so value estimate for $x_{1}$ is lower then for $x_{2}$ \\\n",
        "and $\\Delta w_t = \\alpha (G_t^{\\lambda} - v_w(S_t)) \\nabla_w v_w(S_t) = \\alpha (G_t^{\\lambda} - v_{1}) x_{1} \\mu + \\alpha (G_t^{\\lambda} - v_{2}) x_{2} (1-\\mu) $ \\\n",
        "when $\\pi = \\pi(a | \\cdot) = 0$ this means that the probability of taking action a is 0 so it's never taken, feature $x_{1}$ is not part of the returns and hence is not part of the target \\\n",
        "$G_t^{\\lambda} = \\frac{(\\gamma (1 - \\lambda)  3)}{1 - \\gamma\\lambda} $ \\\n",
        "and on the other hand when $\\pi = \\pi(a | \\cdot) = 1$ this means that the probability of taking action a is 1 so it's always taken and alternative action is never taken, feature $x_{2}$ is not part of the returns and hence is not part of the target \\\n",
        "$G_t^{\\lambda} = \\frac{(\\gamma (1 - \\lambda)  2)}{1 - \\gamma\\lambda} $ \\\n",
        "In case where $\\pi = \\pi(a | \\cdot) = 1$ or 0 the policy is deterministic and the algorithms fails to explore the full action space leading to asymetric results. If a feature is not part of the target return due to policy, than the expected weight update is not being updated in the direction of minimizing the difference between the expected return and the current value estimate and hence the divergence.\\\n",
        "More generally if in the expected return we have value functions weighted by $0 < \\pi < 1$, the choise of $\\pi$ dictates which features are emphathisesd during learning. In this case both $v_{1}$ and $v_{2}$ are part of the expected return weighted by $\\pi$, which corresponds to the probability of taking action a or action b. Since $v_{1}$ < $v_{2}$ , if $\\pi$ does not evenly explore the actions, learning becomes asymentric. \n",
        " \n",
        "4. \n",
        "5. _...answer here..._\n",
        "6. _...answer here..._\n",
        "7. _...answer here..._\n",
        "8. _...answer here..._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhP_7uHmsaUw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3",
        "kind": "private"
      },
      "name": "UCL RL CW 2024, part II",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2021/RL_assignment_2_solutions.ipynb",
          "timestamp": 1645016484699
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1582541397384
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1581969444858
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1581964637124
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2_solutions.ipynb",
          "timestamp": 1581957222796
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2.ipynb?workspaceId=mtthss:ucl::citc",
          "timestamp": 1581683857481
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2020/RL_assignment_2.ipynb",
          "timestamp": 1581608852108
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_2.ipynb",
          "timestamp": 1580904796770
        },
        {
          "file_id": "/piper/depot/google3/learning/deepmind/research/ucl/assignments_2019/RL_assignment_2.ipynb",
          "timestamp": 1548782473207
        },
        {
          "file_id": "1t1PIXqa3m-irLvQ_fQ7qgaBZeTNqiG2v",
          "timestamp": 1542801802962
        },
        {
          "file_id": "1Ldj742iIDtvjYKKwENvrpTQ3Hm2wrqIg",
          "timestamp": 1517862636703
        },
        {
          "file_id": "1FwMxkDPkt68fxovrMmmWwm6ohYvX2wt1",
          "timestamp": 1517660129183
        },
        {
          "file_id": "1wwTq5nociiMHUb26jxrvZvGN6l11xV5o",
          "timestamp": 1517174839485
        },
        {
          "file_id": "1_gJNoj9wG4mnigscGRAcZx7RHix3HCjG",
          "timestamp": 1515086437469
        },
        {
          "file_id": "1hcBeMVfaSh8g1R2ujtmxOSHoxJ8xYkaW",
          "timestamp": 1511098107887
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
